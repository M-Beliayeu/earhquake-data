{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "44c62dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import json\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "\n",
    "#I do not guarantee that the following code is 100% sql injection proof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744777a",
   "metadata": {},
   "source": [
    "<h2>Connect to (MySQL) Server</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0f9d0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"root\"\n",
    "password = \"password\"\n",
    "server = \"localhost\"\n",
    "database = \"quakes\"\n",
    "\n",
    "engine = db.create_engine(f\"mysql+pymysql://{username}:{password}@{server}/{database}\")\n",
    "metadata = db.MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f391c895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/jq5dp98n2pvcf43bjrz71s540000gn/T/ipykernel_55576/4162273999.py:1: SADeprecationWarning: The Engine.table_names() method is deprecated and will be removed in a future release.  Please refer to Inspector.get_table_names(). (deprecated since: 1.4)\n",
      "  engine.table_names()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Agencies',\n",
       " 'Alerts',\n",
       " 'Contributed',\n",
       " 'Events',\n",
       " 'IdMap',\n",
       " 'MagnitudeTypes',\n",
       " 'Status']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f59f3",
   "metadata": {},
   "source": [
    "Create table variable for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fa48d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_tbl = db.Table('Events', metadata, autoload_with=engine)\n",
    "idMap_tbl = db.Table('IdMap', metadata, autoload_with=engine)\n",
    "agencies_tbl = db.Table('Agencies', metadata, autoload_with=engine)\n",
    "status_tbl = db.Table('Status', metadata, autoload_with=engine)\n",
    "alerts_tbl = db.Table('Alerts', metadata, autoload_with=engine)\n",
    "contributed_tbl = db.Table('Contributed', metadata, autoload_with=engine)\n",
    "mag_types_tbl = db.Table('MagnitudeTypes', metadata, autoload_with=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7cceb3",
   "metadata": {},
   "source": [
    "<h2>Read data from files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "28329cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ids(local_id, local_ids, con):\n",
    "    \"\"\"\n",
    "    Inserts (if not present) local_id + local_ids into IdMap table. \n",
    "    Throws an exception if:\n",
    "        1. Some of local_id + local_ids are already assigned to different global ids, e.g. \n",
    "                local_ids contains 'us12345' and 'us67890' with 'us12345' --> 1 and 'us67890' --> 2.\n",
    "        2. There was an event with the same local_id.\n",
    "        \n",
    "        Parameters:\n",
    "            local_id: int --- primary id associated with the event.\n",
    "            local_ids: Iterable[int] --- other ids associated with the event (possibly including local_id).\n",
    "            con: sqlalchemy.engine.Connection --- connection to sql server.\n",
    "            \n",
    "        Output: \n",
    "            (primary_key_to_record_with_local_id: int, global_id_assigned_to_local_id: int).\n",
    "    \"\"\"\n",
    "    \n",
    "    #pk(IdMap) for local_id (or None if it wasn't inserted)\n",
    "    res_id = con.execute(db.select(idMap_tbl.c.id).where(idMap_tbl.c.local_id == local_id)).scalar()\n",
    "    \n",
    "    #throw exception if there was an event with the same local_id\n",
    "    if res_id is not None and \\\n",
    "       con.execute(db.select(events_tbl.c.id).where(events_tbl.c.local_id == res_id)).rowcount > 0:\n",
    "        raise Exception(f\"Duplicate local_id {local_id}.\")\n",
    "    \n",
    "    #local_ids = set of local_id + local_ids\n",
    "    local_ids = set(local_ids)\n",
    "    local_ids.add(local_id)\n",
    "    \n",
    "    #find all global ids corresponding to local ids\n",
    "    globals = list(con.execute(db.select(idMap_tbl.c.global_id).where(idMap_tbl.c.local_id.in_(local_ids)).distinct()))\n",
    "    \n",
    "    #throw if local ids are assigned more than one global id\n",
    "    if len(globals) > 1:\n",
    "        raise Exception('Overlapping is not supported.')\n",
    "    \n",
    "    new_global = None\n",
    "    #if some global id was assigned, get it\n",
    "    if (len(globals) > 0):\n",
    "        new_global = globals[0][0]\n",
    "    #otherwise define new global id as max of all global ids + 1 (if table is empty return 1)\n",
    "    else:\n",
    "        possible_id = con.execute(text(\"SELECT MAX(global_id) FROM IdMap\")).scalar()\n",
    "        new_global = 1 if possible_id is None else possible_id + 1\n",
    "\n",
    "    #insert those local ids into IdMap table that are not already present\n",
    "    for lid in local_ids:\n",
    "        insert_stmt = db.dialects.mysql \\\n",
    "                                 .insert(idMap_tbl).values(local_id=lid, global_id=new_global)\n",
    "        res = con.execute(insert_stmt.on_duplicate_key_update(global_id=insert_stmt.inserted.global_id))   \n",
    "    \n",
    "    #find pk(IdMap) for local_id\n",
    "    id = con.execute(db.select(idMap_tbl.c.id).where(idMap_tbl.c.local_id == local_id)).scalar()\n",
    "    return id, new_global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a21998db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sources(sources, event_global_id, con):\n",
    "    \"\"\"\n",
    "    Updates Contributed table.\n",
    "    Throws an exception if one of sources is not already present in Agencies table.\n",
    "    \n",
    "        Parameters:\n",
    "            sources: Iterable[str] --- agencies that contributed to the event\n",
    "            event_global_id: int --- primary key to the event in question\n",
    "            con: sqlalchemy.engine.Connection --- connection to sql server\n",
    "        \n",
    "        Output:\n",
    "            No output\n",
    "    \"\"\"\n",
    "    \n",
    "    sources = set(sources)\n",
    "\n",
    "    #find pk(Agencies)'s for all sources\n",
    "    stmt = db.select(agencies_tbl.c.id).where(agencies_tbl.c.abbreviation.in_(sources))\n",
    "    res = con.execute(stmt)\n",
    "    \n",
    "    #throw if some agency is not found in Agencies table\n",
    "    if res.rowcount != len(sources):\n",
    "        raise Exception(f'Some agency among {sources} is not found. Add all new agencies to Agencies table.')\n",
    "    \n",
    "    #update Contributed table\n",
    "    select_stmt = db.select(agencies_tbl.c.id, event_global_id).where(agencies_tbl.c.abbreviation.in_(sources))\n",
    "    insert_stmt = db.dialects.mysql.insert(contributed_tbl) \\\n",
    "                            .from_select(['agency', 'event_global_id'], select_stmt)\n",
    "    con.execute(insert_stmt.on_duplicate_key_update(agency=insert_stmt.inserted.agency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75d428f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geojson_entry(entry, con):\n",
    "    \"\"\"\n",
    "    Processes an event (entry) into schema.\n",
    "    Throws if agency(network/net) is null.\n",
    "    \n",
    "        Parameters:\n",
    "            entry: dict --- geojson for the event\n",
    "            con: sqlalchemy.engine.Connection --- connection to sql server\n",
    "        \n",
    "        Output:\n",
    "            No output\n",
    "    \"\"\"\n",
    "    #do nothing if entry is not an earthquake\n",
    "    if entry['properties']['type'] != 'earthquake':\n",
    "        warnings.warn(\"Not an earthquake.\")\n",
    "        return\n",
    "    \n",
    "    local_id = entry['id']\n",
    "    [long, lat, depth] = entry['geometry']['coordinates']\n",
    "    props = entry['properties']\n",
    "    \n",
    "    #process ids\n",
    "    id_IdMap, global_id = process_ids(local_id, filter(None, props['ids'].split(',')), con)\n",
    "    \n",
    "    #process alert, status and magnitude type\n",
    "    alert_id = con.execute(db.select(alerts_tbl.c.id).where(alerts_tbl.c.name == props['alert'])).scalar()\n",
    "    status_id = con.execute(db.select(status_tbl.c.id).where(status_tbl.c.name == props['status'])).scalar()\n",
    "    mag_type_id = con.execute(db.select(mag_types_tbl.c.id).where(mag_types_tbl.c.name == props['magType'])).scalar()\n",
    "    \n",
    "    #find pk(Agencies) for agency\n",
    "    net_id = con.execute(db.select(agencies_tbl.c.id).where(agencies_tbl.c.abbreviation == props['net'])).scalar()\n",
    "    if net_id is None:\n",
    "        raise Exception('Agency is required.')\n",
    "    \n",
    "    #process sources\n",
    "    sources = set(filter(None, props['sources'].split(',')))\n",
    "    sources.add(props['net'])\n",
    "    process_sources(sources, global_id, con)\n",
    "    \n",
    "    #insert event into Events table\n",
    "    con.execute(events_tbl.insert().values(\n",
    "                      magnitude=props['mag'], place=props['place'], \\\n",
    "                      time=datetime.datetime.fromtimestamp(props['time']//1000), \\\n",
    "                      updated=datetime.datetime.fromtimestamp(props['updated']//1000), \\\n",
    "                      timezone_offset=props['tz'], url=props['url'], \\\n",
    "                      detail=props['detail'], felt=props['felt'], cdi=props['cdi'], \\\n",
    "                      mmi=props['mmi'], alert=alert_id, status=status_id, \\\n",
    "                      tsunami=props['tsunami'], significance=props['sig'], network=net_id, \\\n",
    "                      n_stations=props['nst'], dmin=props['dmin'], rms=props['rms'], \\\n",
    "                      gap=props['gap'], magnitude_type=mag_type_id, title=props['title'], \\\n",
    "                      longitude=long, latitude=lat, depth=depth, \\\n",
    "                      local_id=id_IdMap, types=props['types'], code=props['code'] \\\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b6e7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geojson_file(geojson, con):\n",
    "    \"\"\"\n",
    "    Processes a geojson file into schema \n",
    "    \n",
    "        Parameters:\n",
    "            geojson: dict --- geojson with events\n",
    "            con: sqlalchemy.engine.Connection --- connection to sql server\n",
    "        \n",
    "        Output:\n",
    "            No output\n",
    "    \"\"\"\n",
    "    j = 1\n",
    "    records = geojson['features']\n",
    "    for record in records:\n",
    "        process_geojson_entry(record, con)\n",
    "        \n",
    "        if j % 100 == 0:\n",
    "            print(f\"{j} entries processed.\")\n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268635c",
   "metadata": {},
   "source": [
    "<h4>Geojson files:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "784e9cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quakes-00000-20000.json',\n",
       " 'quakes-20000-40000.json',\n",
       " 'quakes-40000-60000.json',\n",
       " 'quakes-60000-80000.json',\n",
       " 'quakes-80000-100000.json']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_prefix = 'quakes-'\n",
    "data_folder = 'data/'\n",
    "\n",
    "all_files = !ls {data_folder}\n",
    "all_files = list(filter(lambda x: x.startswith(file_prefix), all_files))\n",
    "all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239acd43",
   "metadata": {},
   "source": [
    "<h4>Read them into the database</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8493c5ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 entries processed.\n",
      "200 entries processed.\n",
      "300 entries processed.\n",
      "400 entries processed.\n",
      ".................................\n"
     ]
    }
   ],
   "source": [
    "with Session(engine) as session:\n",
    "    for file in all_files:\n",
    "        with open(data_folder + file) as f:\n",
    "            data = json.load(f)\n",
    "            process_geojson_file(data, session)\n",
    "            session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d59d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
